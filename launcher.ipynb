{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a84f200e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd1378c",
   "metadata": {},
   "outputs": [],
   "source": [
    "uppercase_lm_path = '3-gram.pruned.1e-7.arpa'\n",
    "if not os.path.exists(uppercase_lm_path):\n",
    "    with gzip.open(lm_gzip_path, 'rb') as f_zipped:\n",
    "        with open(uppercase_lm_path, 'wb') as f_unzipped:\n",
    "            shutil.copyfileobj(f_zipped, f_unzipped)\n",
    "    print('Unzipped the 3-gram language model.')\n",
    "else:\n",
    "    print('Unzipped .arpa already exists.')\n",
    "\n",
    "lm_path = 'lowercase_3-gram.pruned.1e-7.arpa'\n",
    "if not os.path.exists(lm_path):\n",
    "    with open(uppercase_lm_path, 'r') as f_upper:\n",
    "        with open(lm_path, 'w') as f_lower:\n",
    "            for line in f_upper:\n",
    "                f_lower.write(line.lower())\n",
    "print('Converted language model file to lowercase.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a9a994e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The torchaudio backend is switched to 'soundfile'. Note that 'sox_io' is not supported on Windows.\n",
      "The torchaudio backend is switched to 'soundfile'. Note that 'sox_io' is not supported on Windows.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "None\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'resume_model' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m~\\Documents\\HW\\DLA\\asr\\asr_project_template\\train.py:126\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    119\u001b[0m CustomArgs \u001b[38;5;241m=\u001b[39m collections\u001b[38;5;241m.\u001b[39mnamedtuple(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCustomArgs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflags type target\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    120\u001b[0m options \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    121\u001b[0m     CustomArgs([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--lr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--learning_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m, target\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer;args;lr\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    122\u001b[0m     CustomArgs(\n\u001b[0;32m    123\u001b[0m         [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--bs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--batch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m, target\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_loader;args;batch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    124\u001b[0m     ),\n\u001b[0;32m    125\u001b[0m ]\n\u001b[1;32m--> 126\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[43mConfigParser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    127\u001b[0m main(config)\n",
      "File \u001b[1;32m~\\Documents\\HW\\DLA\\asr\\asr_project_template\\hw_asr\\utils\\parse_config.py:94\u001b[0m, in \u001b[0;36mConfigParser.from_args\u001b[1;34m(cls, args, options)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m# parse custom cli options into dictionary\u001b[39;00m\n\u001b[0;32m     91\u001b[0m modification \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     92\u001b[0m     opt\u001b[38;5;241m.\u001b[39mtarget: \u001b[38;5;28mgetattr\u001b[39m(args, _get_opt_name(opt\u001b[38;5;241m.\u001b[39mflags)) \u001b[38;5;28;01mfor\u001b[39;00m opt \u001b[38;5;129;01min\u001b[39;00m options\n\u001b[0;32m     93\u001b[0m }\n\u001b[1;32m---> 94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(config, resume, \u001b[43mresume_model\u001b[49m, modification)\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'resume_model' referenced before assignment"
     ]
    }
   ],
   "source": [
    "%run -i train.py -c hw_asr/configs/LibriFull.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "099d14f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_checkpoint_path(run_name, epoch=None, checkpoints_dir='saved/models'):\n",
    "    if epoch=None:\n",
    "        name = 'model_best.pth'\n",
    "    else:\n",
    "        name = f'checkpoint-epoch{epoch}.pth'\n",
    "    res = os.path.join(checkpoints_dir, run_name) # go to dir of this run\n",
    "    res = os.path.join(res, os.listdir(res)[-1]) # go to dir of latest run with this name\n",
    "    res = os.path.join(res, name) # add checkpoint\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d9f85b69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'saved/models\\\\no_aug_train\\\\1014_230922\\\\checkpoint-epoch12.pth'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint1_path = get_checkpoint_path('no_aug_train', 12)\n",
    "checkpoint1_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd8fe6fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "$checkpoint1_path\n",
      "$checkpoint1_path\n",
      "Using arpa instead of binary LM file, decoder instantiation might be slow.\n",
      "Alphabet determined to be of regular style.\n",
      "1 (0.0%) records are longer then 20.0 seconds. Excluding them.\n",
      "13243 (46.4%) records are longer then 200 characters. Excluding them.\n",
      "Filtered 13243(46.4%) records  from dataset\n",
      "61 (2.3%) records are longer then 20.0 seconds. Excluding them.\n",
      "292 (10.8%) records are longer then 200 characters. Excluding them.\n",
      "Filtered 292(10.8%) records  from dataset\n",
      "DeepSpeech(\n",
      "  (conv): Conv1d(128, 128, kernel_size=(19,), stride=(2,))\n",
      "  (tail): Sequential(\n",
      "    (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (1): Dropout(p=0.1, inplace=False)\n",
      "    (2): Hardtanh(min_val=0, max_val=20, inplace=True)\n",
      "    (3): Linear(in_features=128, out_features=2048, bias=True)\n",
      "    (4): Dropout(p=0.1, inplace=False)\n",
      "    (5): Hardtanh(min_val=0, max_val=20, inplace=True)\n",
      "    (6): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (7): Dropout(p=0.1, inplace=False)\n",
      "    (8): Hardtanh(min_val=0, max_val=20, inplace=True)\n",
      "  )\n",
      "  (rnn): RNN(2048, 2048, batch_first=True, bidirectional=True)\n",
      "  (head): Sequential(\n",
      "    (0): Hardtanh(min_val=0, max_val=20, inplace=True)\n",
      "    (1): Linear(in_features=2048, out_features=28, bias=True)\n",
      "  )\n",
      ")\n",
      "Trainable parameters: 21615004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3df891ni) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caad3b00acd843ef952fe5412c2cc275",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, maxâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">stoic-yogurt-135</strong>: <a href=\"https://wandb.ai/dlishudi/asr_project/runs/3df891ni\" target=\"_blank\">https://wandb.ai/dlishudi/asr_project/runs/3df891ni</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20221015_230433-3df891ni\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3df891ni). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03375b1f0baf477d828e4476c641859a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\777\\Documents\\HW\\DLA\\asr\\asr_project_template\\wandb\\run-20221015_230508-ts0dit7f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/dlishudi/asr_project/runs/ts0dit7f\" target=\"_blank\">vivid-donkey-136</a></strong> to <a href=\"https://wandb.ai/dlishudi/asr_project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$checkpoint1_path\n",
      "$checkpoint1_path\n",
      "Loading model form checkpoint: $checkpoint1_path ...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '$checkpoint1_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m~\\Documents\\HW\\DLA\\asr\\asr_project_template\\train.py:127\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    120\u001b[0m options \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    121\u001b[0m     CustomArgs([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--lr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--learning_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m, target\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer;args;lr\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    122\u001b[0m     CustomArgs(\n\u001b[0;32m    123\u001b[0m         [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--bs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--batch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m, target\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_loader;args;batch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    124\u001b[0m     ),\n\u001b[0;32m    125\u001b[0m ]\n\u001b[0;32m    126\u001b[0m config \u001b[38;5;241m=\u001b[39m ConfigParser\u001b[38;5;241m.\u001b[39mfrom_args(args, options)\n\u001b[1;32m--> 127\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\HW\\DLA\\asr\\asr_project_template\\train.py:60\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m     57\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39minit_obj(config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer\u001b[39m\u001b[38;5;124m\"\u001b[39m], torch\u001b[38;5;241m.\u001b[39moptim, trainable_params)\n\u001b[0;32m     58\u001b[0m lr_scheduler \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39minit_obj(config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr_scheduler\u001b[39m\u001b[38;5;124m\"\u001b[39m], torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler, optimizer)\n\u001b[1;32m---> 60\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_encoder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_encoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctc_decoder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctc_decoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataloaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlen_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrainer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlen_epoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[1;32m~\\Documents\\HW\\DLA\\asr\\asr_project_template\\hw_asr\\trainer\\trainer.py:41\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[1;34m(self, model, criterion, metrics, optimizer, config, device, dataloaders, text_encoder, ctc_decoder, lr_scheduler, len_epoch, skip_oom)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     27\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     28\u001b[0m         model,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     39\u001b[0m         skip_oom\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     40\u001b[0m ):\n\u001b[1;32m---> 41\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_oom \u001b[38;5;241m=\u001b[39m skip_oom\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_encoder \u001b[38;5;241m=\u001b[39m text_encoder\n",
      "File \u001b[1;32m~\\Documents\\HW\\DLA\\asr\\asr_project_template\\hw_asr\\base\\base_trainer.py:56\u001b[0m, in \u001b[0;36mBaseTrainer.__init__\u001b[1;34m(self, model, criterion, metrics, optimizer, config, device)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28mprint\u001b[39m(config\u001b[38;5;241m.\u001b[39mresume_model)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mresume_model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 56\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_resume_model_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresume_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mresume \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resume_checkpoint(config\u001b[38;5;241m.\u001b[39mresume)\n",
      "File \u001b[1;32m~\\Documents\\HW\\DLA\\asr\\asr_project_template\\hw_asr\\base\\base_trainer.py:205\u001b[0m, in \u001b[0;36mBaseTrainer._resume_model_checkpoint\u001b[1;34m(self, resume_path)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28mprint\u001b[39m(resume_path)\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading model form checkpoint: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m ...\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(resume_path))\n\u001b[1;32m--> 205\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;66;03m# load architecture params from checkpoint.\u001b[39;00m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m checkpoint[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124march\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124march\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:699\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    696\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    697\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 699\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m    701\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m    702\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m    703\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m    704\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:231\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    230\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 231\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:212\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 212\u001b[0m     \u001b[38;5;28msuper\u001b[39m(_open_file, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '$checkpoint1_path'"
     ]
    }
   ],
   "source": [
    "%run -i train.py --resume_model $checkpoint1_path --config hw_asr/configs/LibriFull.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d894e609",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import kenlm\n",
    "import numpy as np\n",
    "\n",
    "str1 = 'my old friend was not there'.upper()\n",
    "str2 = 'my old frieqnd was nfsdfot there'.upper()\n",
    "\n",
    "\n",
    "model = kenlm.Model('./lm_models/3gpruned.arpa')\n",
    "# print(f'{str1} | {np.exp(model.score(str1, bos = True, eos = False))}')\n",
    "for a in [False, True]:\n",
    "    for b in [False, True]:\n",
    "        print(a, b)\n",
    "        print(f'{str1} | {np.exp(model.score(str1, bos = a, eos = b))}')\n",
    "        print(f'{str2} | {np.exp(model.score(str2, bos = a, eos = b))}')\n",
    "        print(np.exp(model.score(str1, bos = a, eos = b) - model.score(str2, bos = a, eos = b)))\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0571f1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda env list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
